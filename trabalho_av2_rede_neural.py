# -*- coding: utf-8 -*-
"""Trabalho AV2 - Rede Neural.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gbw7crxPvr3rdVxwX-7CI79teEYVALGk
"""

# Etapa 0 ‚Äî Instala√ß√£o e imports

import os
import json
import numpy as np
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.neural_network import MLPRegressor
import joblib

# Etapa 1 ‚Äî Leitura do arquivo de dados
DATA_DIR = Path("/content")

# Busca autom√°tica do arquivo
candidates = [
    DATA_DIR / "consumo_energia_full.csv",
    DATA_DIR / "consumo_energia_train.csv",
    DATA_DIR / "consumo_energia_test.csv",
    DATA_DIR / "consumo_energia.csv",
]

csv_path = next((p for p in candidates if p.exists()), None)
if csv_path is None:
    raise FileNotFoundError("Nenhum arquivo CSV encontrado. Envie um arquivo de entrada!")

print(f"‚úÖ Usando arquivo: {csv_path.name}")

df = pd.read_csv(csv_path)
df.head(10)

# Etapa 2 ‚Äî Explora√ß√£o de dados
expected_cols = ["x1", "x2", "x3", "y"]
df = df[expected_cols].dropna().reset_index(drop=True)

# Estat√≠sticas descritivas
desc = df.describe().T
display(desc)

# Criar pasta para salvar gr√°ficos
PLOTS_DIR = DATA_DIR / "mlp_energy_plots"
PLOTS_DIR.mkdir(exist_ok=True)

# Histogramas
for col in expected_cols:
    plt.figure(figsize=(6,4))
    plt.hist(df[col], bins=30)
    plt.title(f"Histograma: {col}")
    plt.xlabel(col)
    plt.ylabel("Frequ√™ncia")
    plt.tight_layout()
    plt.savefig(PLOTS_DIR / f"hist_{col}.png")
    plt.close()

# Scatterplots x1,x2,x3 vs y
for col in ["x1","x2","x3"]:
    plt.figure(figsize=(6,4))
    plt.scatter(df[col], df["y"], alpha=0.6)
    plt.title(f"{col} vs y")
    plt.xlabel(col)
    plt.ylabel("Consumo (y)")
    plt.tight_layout()
    plt.savefig(PLOTS_DIR / f"scatter_{col}_y.png")
    plt.close()

# Matriz de correla√ß√£o
corr = df.corr()
display(corr)

plt.figure(figsize=(5,4))
plt.imshow(corr.values)
plt.xticks(range(len(corr.columns)), corr.columns)
plt.yticks(range(len(corr.index)), corr.index)
plt.colorbar()
plt.title("Matriz de Correla√ß√£o")
plt.tight_layout()
plt.savefig(PLOTS_DIR / "correlation_matrix.png")
plt.close()

# Etapa 3 ‚Äî Pr√©-processamento
X = df[["x1", "x2", "x3"]].values
y = df["y"].values.reshape(-1,1)

# Split 80/20
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normaliza√ß√£o
scaler_X = StandardScaler()
scaler_y = StandardScaler()

X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)
y_train_scaled = scaler_y.fit_transform(y_train).ravel()
y_test_scaled = scaler_y.transform(y_test).ravel()

# Salvar scalers
joblib.dump(scaler_X, "scaler_X.pkl")
joblib.dump(scaler_y, "scaler_y.pkl")

print("‚úÖ Normaliza√ß√£o conclu√≠da e scalers salvos.")

# Etapa 4 ‚Äî Constru√ß√£o e Treinamento do Modelo
mlp = MLPRegressor(
    hidden_layer_sizes=(10,),
    activation='relu',
    solver='adam',
    max_iter=300,
    random_state=42,
    early_stopping=True,
    validation_fraction=0.1,
    n_iter_no_change=20,
    tol=1e-4,
    verbose=True
)

mlp.fit(X_train_scaled, y_train_scaled)

# Salvar modelo
joblib.dump(mlp, "mlp_energy_model_joblib.pkl")

# Curva de perda
plt.figure(figsize=(6,4))
plt.plot(mlp.loss_curve_)
plt.xlabel("Itera√ß√µes")
plt.ylabel("Loss")
plt.title("Curva de perda (MLPRegressor)")
plt.tight_layout()
plt.savefig(PLOTS_DIR / "loss_curve.png")
plt.show()

# Etapa 5 ‚Äî Avalia√ß√£o
y_pred_scaled = mlp.predict(X_test_scaled)
y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1,1))
y_test_orig = y_test

mse_val = mean_squared_error(y_test_orig, y_pred)
mae_val = mean_absolute_error(y_test_orig, y_pred)
r2_val = r2_score(y_test_orig, y_pred)

print("üìà M√©tricas de Avalia√ß√£o:")
print(f"MSE: {mse_val:.4f}")
print(f"MAE: {mae_val:.4f}")
print(f"R¬≤:  {r2_val:.4f}")

# Salvar m√©tricas
metrics = {"MSE": mse_val, "MAE": mae_val, "R2": r2_val}
with open("evaluation_metrics.json", "w") as f:
    json.dump(metrics, f, indent=2)

# Gr√°fico y_real vs y_previsto
plt.figure(figsize=(6,6))
plt.scatter(y_test_orig, y_pred, alpha=0.6)
minv = min(y_test_orig.min(), y_pred.min())
maxv = max(y_test_orig.max(), y_pred.max())
plt.plot([minv, maxv], [minv, maxv], "r--")
plt.xlabel("y_real")
plt.ylabel("y_previsto")
plt.title("y_real vs y_previsto")
plt.tight_layout()
plt.savefig(PLOTS_DIR / "y_real_vs_y_pred.png")
plt.show()

# Distribui√ß√£o dos res√≠duos
residuals = (y_pred - y_test_orig).ravel()
plt.figure(figsize=(6,4))
plt.hist(residuals, bins=30)
plt.title("Distribui√ß√£o dos res√≠duos (y_pred - y_true)")
plt.xlabel("Res√≠duo")
plt.ylabel("Frequ√™ncia")
plt.tight_layout()
plt.savefig(PLOTS_DIR / "residuals_hist.png")
plt.show()

# Res√≠duos vs previs√£o
plt.figure(figsize=(6,4))
plt.scatter(y_pred, residuals, alpha=0.6)
plt.axhline(0, linestyle="--", color="r")
plt.xlabel("y_previsto")
plt.ylabel("res√≠duo")
plt.title("Res√≠duos vs y_previsto")
plt.tight_layout()
plt.savefig(PLOTS_DIR / "residuals_vs_pred.png")
plt.show()

# Etapa 6 ‚Äî Relat√≥rio e salvamento final
report_md = f"""
# Relat√≥rio r√°pido ‚Äî MLP (sklearn) para estimativa de consumo energ√©tico

**Arquivo usado:** {csv_path.name}

## Modelo
- Topologia: 3 entradas ‚Üí 10 neur√¥nios (ReLU) ‚Üí 1 sa√≠da
- Otimizador: Adam
- Fun√ß√£o de perda: MSE
- Early Stopping: ativo (10% valida√ß√£o)

## M√©tricas (teste)
- MSE: {mse_val:.4f}
- MAE: {mae_val:.4f}
- R¬≤: {r2_val:.4f}

## Arquivos gerados
- Modelo: mlp_energy_model_joblib.pkl
- Scalers: scaler_X.pkl, scaler_y.pkl
- M√©tricas: evaluation_metrics.json
- Gr√°ficos: /content/mlp_energy_plots/
"""

with open("report.md", "w") as f:
    f.write(report_md)

print("‚úÖ Relat√≥rio salvo como report.md")
!ls -lh /content | grep .pkl